# -*- coding: utf-8 -*-
"""Notebook_reseñas_1_Analisis_Exploratorio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1saGQ_QQEj3Q_MRu2UQLa8jVc-k6Rk-0l

# Trabajo Final Inteligencia Artificial

## Grupo: Cecilia Belen Perez Colasanto, Jose María Klappenbach, María Pilar Artigau, Carolina Picciafuoco, Sofía Zubillaga
"""

!pip install datasets

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Los datos en bruto de Yelp abarcan una gran cantidad de información de la plataforma de Yelp, detallando reseñas de usuarios, calificaciones de negocios y especificaciones operativas de una diversa gama de establecimientos locales.

Para ser más específicos, el conjunto de datos en bruto de Yelp contiene cinco datasets JSON diferentes:

**yelp_academic_dataset_business.json** (118.9MB) Este archivo contiene información sobre los negocios listados en Yelp. Cada registro en este dataset típicamente incluye el nombre del negocio, dirección, ciudad, estado, código postal, latitud y longitud, estrellas (calificación promedio), conteo de reseñas, categorías (por ejemplo, Restaurantes, Compras, etc.) y otros atributos como disponibilidad de estacionamiento o si es accesible para sillas de ruedas.

**yelp_academic_dataset_checkin.json** (287MB) El archivo de registros de entrada (check-in) proporciona datos sobre las visitas de los usuarios a los negocios a lo largo del tiempo. Incluye el ID del negocio y una serie de marcas de tiempo que indican cuándo los usuarios hicieron check-in en ese lugar, ofreciendo información sobre la popularidad del negocio en diferentes momentos y días.

**yelp_academic_dataset_review.json** (5.34GB) Este dataset contiene reseñas escritas por usuarios para los negocios. Cada reseña incluye el ID del usuario, el ID del negocio, las estrellas otorgadas (de 1 a 5), votos útiles/divertidos/geniales, el texto de la reseña y la fecha en que se publicó. Estos datos pueden utilizarse para analizar el sentimiento del cliente, evaluar la calidad del servicio y más.

**yelp_academic_dataset_tip.json** (180.6MB) Las recomendaciones son mensajes cortos dejados por los usuarios sobre un negocio, a menudo conteniendo sugerencias, cumplidos o consejos para futuros clientes. Este archivo incluye información como el texto de la recomendación, la fecha en que se dejó, el ID del negocio y el ID del usuario. Las recomendaciones proporcionan retroalimentación rápida e informativa sobre un negocio.

**yelp_academic_dataset_user.json** (3.36GB) Este archivo contiene datos sobre los usuarios de Yelp, incluyendo su ID de usuario, nombre, conteo de reseñas, desde cuándo están en Yelp (la fecha en que se unieron), amigos (una lista de IDs de usuario que representan sus amigos en Yelp), conteos de votos útiles/divertidos/geniales que han recibido, fans (el número de usuarios que los han marcado como "fan") y su promedio de estrellas otorgadas. Puede usarse para analizar el comportamiento de los usuarios, redes sociales e influencia en Yelp.

## Variables:

- **`business_id`**: Un identificador único para cada negocio listado en el conjunto de datos. (no nulo, objeto)
- **`name`**: El nombre del negocio. (no nulo, objeto)
- **`address`**: La dirección del negocio. (no nulo, objeto)
- **`city`**: La ciudad donde se encuentra el negocio. (no nulo, objeto)
- **`state`**: El estado o región donde se encuentra el negocio. (no nulo, objeto)
- **`postal_code`**: El código postal asociado a la ubicación del negocio. (no nulo, objeto)
- **`latitude`**: La coordenada de latitud del negocio para el mapeo geográfico. (no nulo, float64)
- **`longitude`**: La coordenada de longitud del negocio para el mapeo geográfico. (no nulo, float64)
- **`stars_x`**: La calificación promedio de estrellas del negocio. (no nulo, float64)
- **`review_count`**: El número de reseñas que ha recibido el negocio. (no nulo, int64)
- **`is_open`**: Una variable binaria que indica si el negocio está abierto (1) o cerrado (0). (no nulo, int64)
- **`attributes`**: Una colección de atributos sobre el negocio, como 'Acepta tarjetas de crédito', 'Estacionamiento', 'Wi-Fi', etc. (con valores faltantes en 493 filas de un total de 200,000 filas, objeto)
- **`categories`**: Las categorías en las que se encuentra el negocio, como 'Restaurantes', 'Comida', 'Café', etc. (no nulo, objeto)
- **`hours`**: El horario de funcionamiento del negocio. (con valores faltantes en 6,905 filas de un total de 200,000 filas, objeto)
- **`review_id`**: Un identificador único para cada reseña. (no nulo, objeto)
- **`user_id`**: Un identificador único para cada usuario que ha dejado una reseña. (no nulo, objeto)
- **`stars_y`**: La calificación de estrellas otorgada por el usuario en su reseña. (no nulo, float64)
- **`useful`**: El número de usuarios que encontraron útil la reseña. (no nulo, int64)
- **`funny`**: El número de usuarios que encontraron divertida la reseña. (no nulo, int64)
- **`cool`**: El número de usuarios que encontraron genial la reseña. (no nulo, int64)
- **`text`**: El contenido textual de la reseña. (no nulo, objeto)
- **`date`**: La fecha en que se publicó la reseña. (no nulo, objeto)

# Cargamos el Dataset
"""

from datasets import load_dataset

ds = load_dataset("Johnnyeee/Yelpdata_663", split="train")
print(ds)

df = ds.to_pandas()
df.head()

"""# **Filtrado**"""

print(f"El dataset tiene {len(df)} filas y {len(df.columns)} columnas")

"""Como nuesto dataset tiene demasiadas filas con datos, para simplificar el analisis y que el codigo corra con mayor rapides, filtraremos por una ciudad."""

df['city'].nunique()

df.groupby(by="city").count().sort_values("text", ascending = False).head(20)

"""Nos quedamos con la ciudad de Brandon, que es una de las que contienen mayor cantidad de reseñas."""

df_sb = df[df['city'] == "Brandon"]
df_sb.head()

"""Ahora, dado que tenemos una unica ciudad y estado, podemos eliminar las columnas de `state` y de `city`."""

df_sb.drop(columns = ['postal_code',"state", "city"], inplace = True)
df_sb.head()

print(f"Tenemos {df_sb['review_id'].duplicated().sum()} reseñas duplicadas")

print(f"Tenemos {df_sb[['business_id']].duplicated().sum()} filas duplicadas")

# Verificar si cada business_id tiene solo un name
business_id_counts = df_sb.groupby('business_id')['name'].nunique().reset_index()
business_id_counts.columns = ['business_id', 'unique_names']

# Verificar si cada name está asociado a un solo business_id
name_counts = df_sb.groupby('name')['business_id'].nunique().reset_index()
name_counts.columns = ['name', 'unique_business_ids']

business_id_counts_sorted = business_id_counts.sort_values(by='unique_names', ascending=False)
name_counts_sorted = name_counts.sort_values(by='unique_business_ids', ascending=False)

business_id_counts_sorted

name_counts_sorted

"""Como podemos ver, hay restaurantes que tienen mas de un business id.

Veamos si hay filas duplicadas.

Filtramos un por un restaurante y podmeos ver que por ejemplo McDonald's tiene mas de un bussines id porque tiene multiples sucursales. Esto es logico dado que es una franquicia.
"""

df_sb[df_sb.name == "McDonald's"]

df_sb.groupby("business_id").count().sort_values('text', ascending = False)

"""Nos quedamos con la sucursal que más reseñas tenga"""

# Contar las apariciones de cada combinación de 'name' y 'business_id'
counts = df_sb.groupby(['name', 'business_id']).size().reset_index(name='counts')

# Seleccionar la sucursal con más apariciones para cada franquicia
idx_max_counts = counts.groupby('name')['counts'].idxmax()
max_counts = counts.loc[idx_max_counts]

# Filtrar el DataFrame original para quedarte con las sucursales seleccionadas
# Use df_ph instead of df to create the boolean index
df_filtered = df_sb[df_sb.set_index(['name', 'business_id']).index.isin(max_counts.set_index(['name', 'business_id']).index)]
df_filtered.head()

"""Verificamos que ahora hay solo un único restaurante, el que mayor cantidad de reseñas tiene.

Por otro lado, filtraremos por aquellos restaurantes que tengan mas de 100 reseñas, como para que las mismas sean representativas a la hora de evaluar el lugar.
"""

df_filtered[df_filtered['name'] == "McDonald's"]['business_id'].nunique()

# Paso 1: Agrupar por 'business_id' y contar los registros
counts = df_sb.groupby('business_id').size()

# Paso 2: Filtrar los 'business_id' con más de 100 registros
filtered_ids = counts[counts > 100].index

# Paso 3: Usar estos 'business_id' para filtrar el DataFrame original
df_filtered = df_sb[df_sb['business_id'].isin(filtered_ids)]

# Mostrar el DataFrame resultante
df_filtered

"""# **Preprocesamiento I de los datos**
- Limpieza de variables irrelevantes para el modelado
- Creacion de variables: separacion por categorias segun ranking

## **Analisis Exploratorio**
"""

print(f"El dataset que vamos a usar tiene {len(df_filtered)} filas y {len(df_filtered.columns)} columnas")

print("Veamos las columnas:")
for columna in df_filtered.columns:
    print(f"- {columna}")

"""**Informacion general:**"""

df_filtered.info()

df_filtered.describe().round(2)

"""**Veamos los valores nulos:**"""

print("Porcentaje de nulos por variable en df:")
(df_filtered.isnull().sum().sort_values()/len(df_filtered))*100

"""**- Variables Numericas**"""

# Numericas
variables_numericas = df_filtered.select_dtypes(include=['int', 'float'])
print("Veamos las variables numéricas del dataset:")
for columna in variables_numericas.columns:
    print(f"- {columna}")

"""**- Variables Categoricas**"""

# Objeto
df.select_dtypes(include=['object'])
variables_categoricas = df_filtered.select_dtypes(include=['object'])
print("Veamos las variables categoricas del dataset:")
for columna in variables_categoricas.columns:
    print(f"- {columna}")

"""### **stars_x**
La calificación promedio de estrellas del negocio.

Renombramos la columna para que el nombre de la variable sea mas acorde a su informacion.
"""

df_filtered.rename(columns = {"stars_x": "stars_restaurant"}, inplace = True)

variable = 'stars_restaurant'

fig, ax = plt.subplots()
fig.set_size_inches(12, 8)
sns.countplot(x = f'{variable}', data = df_filtered)
sns.countplot(x=variable, data=df_filtered, palette='viridis', ax=ax)
ax.set_xlabel(f'{variable}', fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable} Count Distribution', fontsize=15, weight="bold")
sns.despine()

variable = 'stars_restaurant'

print(f'Min {variable}: ', df_filtered[f'{variable}'].min())
print(f'Max {variable}: ', df_filtered[f'{variable}'].max())
print(f'Promedio {variable}: ', df_filtered[f'{variable}'].mean().round(2))

"""Las calificaciones por restaurante va desde 1 a 5, pero no hay restaurantes con clasificacion de 1 estrella. El promedio de estrellas entre restaurantes es de 3.81 y se puede ver que la mayoria de restaurantes tienen 3.5, 4 y 4,5 estrellas. Hay pocos restaurantes con 5 estrellas."""

variable = 'stars_restaurant'

fig, ax = plt.subplots(1,2,figsize = (10,4))
ax[0].set_title(f"Distribucion de {variable}", weight="bold")
sns.histplot(data = df_filtered, x = f"{variable}", kde = True, ax = ax[0], color='skyblue')
ax[1].set_title(f"Boxplot de {variable}", weight="bold")
sns.boxplot(data = df_filtered, x = f"{variable}", ax= ax[1], color='lightcoral')

"""Los que parecieran ser outliers segun el boxplot claramente no lo son, ya que las estrellas van de 0 a 5, por lo que los datos son correctos y no atipicos.

### **review_count**
El número de reseñas que ha recibido el negocio.
"""

df_filtered.groupby("name").review_count.sum().sort_values(ascending = False)

df_filtered.groupby("name").review_count.sum().sort_values(ascending = False).mean()

"""Se puede ver que la cantidad de reseñas por restaurante van desde 697.344 reseñas a 13.054. Dado que nosotros cortamos la base de datos, esta cantidad de reseñas no figura en nuestro dataset, es por ello que al ser irrelevante esta variable, optamos por eliminarla."""

df_filtered.drop(columns = ['review_count'], inplace = True)

df_filtered.name.value_counts()

df_filtered.name.value_counts().mean()

"""En nuestro dataset de la ciudad de Brandon, el restaurante que mas reseñas tiene es de 768, y el que menos tiene cuenta con 106. El promedio es de 205 reseñas por negocio.

### **is_open**
Una variable binaria que indica si el negocio está abierto (1) o cerrado (0).
"""

df_filtered.is_open.value_counts()

"""Dado que la variable indica si el lugar se encuentra abierto o cerrado pero eso depende del dia y horario, esta variable tampoco resulta relevante, por lo que tambien se opta por eliminarla."""

df_filtered.drop(columns = ['is_open'], inplace = True)

"""### **stars_y**
La calificación de estrellas otorgada por el usuario en su reseña.

Renombramos la columna para que el nombre de la variable sea mas acorde a su informacion.
"""

df_filtered.rename(columns = {"stars_y": "stars_review"}, inplace = True)

variable = 'stars_review'

fig, ax = plt.subplots()
fig.set_size_inches(12, 8)
sns.countplot(x = f'{variable}', data = df_filtered)
sns.countplot(x=variable, data=df_filtered, palette='viridis', ax=ax)
ax.set_xlabel(f'{variable}', fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable} Count Distribution', fontsize=15, weight="bold")
sns.despine()

variable = 'stars_review'

print(f'Min {variable}: ', df_filtered[f'{variable}'].min())
print(f'Max {variable}: ', df_filtered[f'{variable}'].max())
print(f'Promedio {variable}: ', df_filtered[f'{variable}'].mean().round(2))

"""Las calificaciones por restaurante van desde 1 a 5. El promedio de estrellas es de 3.81. Hay un numero muy alto de reseñas con 5 estrellas."""

variable = 'stars_review'

fig, ax = plt.subplots(1,2,figsize = (10,4))
ax[0].set_title(f"Distribucion de {variable}", weight="bold")
sns.histplot(data = df_filtered, x = f"{variable}", kde = True, ax = ax[0], color='skyblue')
ax[1].set_title(f"Boxplot de {variable}", weight="bold")
sns.boxplot(data = df_filtered, x = f"{variable}", ax= ax[1], color='lightcoral')

"""### **useful**
El número de usuarios que encontraron útil la reseña.
"""

variable = 'useful'

fig, ax = plt.subplots(1,2,figsize = (10,4))
ax[0].set_title(f"Distribucion de {variable}", weight="bold")
sns.histplot(data = df_filtered, x = f"{variable}", kde = True, ax = ax[0], color='skyblue')
ax[1].set_title(f"Boxplot de {variable}", weight="bold")
sns.boxplot(data = df_filtered, x = f"{variable}", ax= ax[1], color='lightcoral')

variable = 'useful'

print(f'Min {variable}: ', df_filtered[f'{variable}'].min())
print(f'Max {variable}: ', df_filtered[f'{variable}'].max())
print(f'Promedio {variable}: ', df_filtered[f'{variable}'].mean().round(2))


print('\n1º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.25))
print('2º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.50))
print('3º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.75))
print('4º Quartile: ', df_filtered[f"{variable}"].quantile(q = 1.00))

print(f'\nValores de "{variable}s" mayores a', df[f"{variable}"].quantile(q = 0.75) +
                      1.5 * (df[f"{variable}"].quantile(q = 0.75) - df[f"{variable}"].quantile(q = 0.25)), 'se pueden considerar outliers')

df_filtered[df_filtered.useful == df_filtered.useful.max()]

"""La reseña que fue encontrada mas util fue votada asi por 59 usuarios."""

df_filtered.groupby("name").useful.sum().sort_values(ascending = False).head(5)

"""Los restaurantes que cuentan con mas reseñas utiles son The Stein & Vine,
Ford's Garage, Moreno Bakery y Portillo's Hot Dogs con arriba de los 500 puntos de useful totales en las reseñas de los mismos.

### **funny**
El número de usuarios que encontraron divertida la reseña.
"""

variable = 'funny'

fig, ax = plt.subplots(1,2,figsize = (10,4))
ax[0].set_title(f"Distribucion de {variable}", weight="bold")
sns.histplot(data = df_filtered, x = f"{variable}", kde = True, ax = ax[0], color='skyblue')
ax[1].set_title(f"Boxplot de {variable}", weight="bold")
sns.boxplot(data = df_filtered, x = f"{variable}", ax= ax[1], color='lightcoral')

variable = 'funny'

print(f'Min {variable}: ', df_filtered[f'{variable}'].min())
print(f'Max {variable}: ', df_filtered[f'{variable}'].max())
print(f'Promedio {variable}: ', df_filtered[f'{variable}'].mean().round(2))


print('\n1º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.25))
print('2º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.50))
print('3º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.75))
print('4º Quartile: ', df_filtered[f"{variable}"].quantile(q = 1.00))

print(f'\nValores de "{variable}s" mayores a', df[f"{variable}"].quantile(q = 0.75) +
                      1.5 * (df[f"{variable}"].quantile(q = 0.75) - df[f"{variable}"].quantile(q = 0.25)), 'se pueden considerar outliers')

df_filtered[df_filtered.funny == df_filtered.funny.max()]

"""Esta fue la reseña votada como mas graciosa, que da la coincidencia que es la misma que fue clasificada como la mas util."""

df_filtered.groupby("name").funny.sum().sort_values(ascending = False).head(5)

"""Los restaurantes que cuentan con mas reseñas graciosas son Portillo's Hot Dogs, The Stein & Vine, Moreno Bakery y Ford's Garage con arriba de los 100 puntos de funny totales en las reseñas de los mismos.

De igual manera, para nuestro analisis no resulta relevante saber que tan graciosa fue la reseña por lo que optamos por eliminarla.
"""

df_filtered.drop(columns = ['funny'], inplace = True)

"""### **cool**
El número de usuarios que encontraron genial la reseña.
"""

variable = 'cool'

fig, ax = plt.subplots(1,2,figsize = (10,4))
ax[0].set_title(f"Distribucion de {variable}", weight="bold")
sns.histplot(data = df_filtered, x = f"{variable}", kde = True, ax = ax[0], color='skyblue')
ax[1].set_title(f"Boxplot de {variable}", weight="bold")
sns.boxplot(data = df_filtered, x = f"{variable}", ax= ax[1], color='lightcoral')

variable = 'cool'

print(f'Min {variable}: ', df_filtered[f'{variable}'].min())
print(f'Max {variable}: ', df_filtered[f'{variable}'].max())
print(f'Promedio {variable}: ', df_filtered[f'{variable}'].mean().round(2))

print('\n1º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.25))
print('2º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.50))
print('3º Quartile: ', df_filtered[f"{variable}"].quantile(q = 0.75))
print('4º Quartile: ', df_filtered[f"{variable}"].quantile(q = 1.00))

print(f'\nValores de "{variable}s" mayores a', df[f"{variable}"].quantile(q = 0.75) +
                      1.5 * (df[f"{variable}"].quantile(q = 0.75) - df[f"{variable}"].quantile(q = 0.25)), 'se pueden considerar outliers')

df_filtered[df_filtered.cool == df_filtered.cool.max()]

"""De igual manera, para nuestro analisis no resulta relevante saber que tan cool fue la reseña por lo que optamos por eliminarla al igual que funny."""

df_filtered.drop(columns = ['cool'], inplace = True)

"""### **reviw_id**"""

df_filtered.review_id.nunique()

"""### **name**
El nombre del negocio.
"""

df_filtered.name.value_counts()

"""Como se menciono anteriormente en el analisis de la variable `review_count`, en nuestro dataset de la ciudad de Brandon, el restaurante que mas reseñas tiene es de 768, y el que menos tiene cuenta con 106. El promedio es de 205 reseñas por negocio.  """

df_filtered.name.nunique()

"""### **categories**

Como la variable se encuentra en diccionarios, vamos a separar las categorias en columnas para poder utilizarlas.
"""

categories_count = df_filtered['categories'].str.split(',').explode().str.strip().value_counts()
categories_count

"""Eliminaremos la categorias 'Restaurants' ya que es un valor que se repite en casi todas las filas, y además el dataset de por si es sobre restaurantes. Lo mismo sucede con la categoria 'Food'.Es una categoria que no nos aporta información."""

# Eliminar categorías no importantes
categories_to_remove = ['Restaurants', 'American (New)', 'American (Traditional)', 'Food', 'Bars' ]

categories_count = categories_count.drop(categories_to_remove)
# Crear una lista ordenada de categorías
ordered_cat_list = categories_count.index.tolist()

def ordenar_genero(categories):
    if isinstance(categories, str):
        split_cat = [g.strip() for g in categories.split(',') if g not in categories_to_remove]
        sorted_cat = sorted(split_cat, key=lambda g: ordered_cat_list.index(g) if g in ordered_cat_list else len(ordered_cat_list))
        return ', '.join(sorted_cat)
    return ''

# Crear la nueva columna con las categorías filtradas y ordenadas
df_filtered['sorted_categories'] = df_filtered['categories'].apply(ordenar_genero)

# Separar las categorías ordenadas en columnas individuales
# Use df_filtered instead of df here
split_categories = df_filtered['sorted_categories'].str.split(', ', expand=True)
df_filtered['cat_1'] = split_categories[0]
df_filtered['cat_2'] = split_categories[1]
df_filtered['cat_3'] = split_categories[2]
df_filtered['cat_4'] = split_categories[3]

df_filtered

df_filtered.isnull().sum()

df_filtered.dropna(inplace = True)

df_filtered.nunique()

variable = 'cat_1'

order = df_filtered[variable].value_counts().index

fig, ax = plt.subplots()
fig.set_size_inches(20, 8)

# Graficar el countplot
sns.countplot(x=variable, data=df_filtered, palette='hls', ax=ax, order = order)

ax.grid(True, which='both', linestyle='--', linewidth=0.7)
ax.set_axisbelow(True)

ax.set_xlabel(f"{variable}", fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable.capitalize()} Count Distribution', fontsize=15, weight="bold")
ax.tick_params(labelsize=15)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

sns.despine()
plt.show()

variable = 'cat_2'

order = df_filtered[variable].value_counts().index

fig, ax = plt.subplots()
fig.set_size_inches(20, 8)

# Graficar el countplot
sns.countplot(x=variable, data=df_filtered, palette='hls', ax=ax, order = order)

ax.grid(True, which='both', linestyle='--', linewidth=0.7)
ax.set_axisbelow(True)

ax.set_xlabel(f"{variable}", fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable.capitalize()} Count Distribution', fontsize=15, weight="bold")
ax.tick_params(labelsize=15)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

sns.despine()
plt.show()

variable = 'cat_3'

order = df_filtered[variable].value_counts().index

fig, ax = plt.subplots()
fig.set_size_inches(20, 8)

# Graficar el countplot
sns.countplot(x=variable, data=df_filtered, palette='hls', ax=ax, order = order)

ax.grid(True, which='both', linestyle='--', linewidth=0.7)
ax.set_axisbelow(True)

ax.set_xlabel(f"{variable}", fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable.capitalize()} Count Distribution', fontsize=15, weight="bold")
ax.tick_params(labelsize=15)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

sns.despine()
plt.show()

variable = 'cat_4'

order = df_filtered[variable].value_counts().index

fig, ax = plt.subplots()
fig.set_size_inches(20, 8)

# Graficar el countplot
sns.countplot(x=variable, data=df_filtered, palette='hls', ax=ax, order = order)

ax.grid(True, which='both', linestyle='--', linewidth=0.7)
ax.set_axisbelow(True)

ax.set_xlabel(f"{variable}", fontsize=15)
ax.set_ylabel('Count', fontsize=15)
ax.set_title(f'{variable.capitalize()} Count Distribution', fontsize=15, weight="bold")
ax.tick_params(labelsize=15)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

sns.despine()
plt.show()

"""### **hours**
El horario de funcionamiento del negocio.
"""

df_filtered['hours']

"""### **date**
La fecha en que se publicó la reseña.
"""

df_filtered.date

# Crear una nueva columna temporal con solo los años
df_filtered['year'] = pd.to_datetime(df_filtered['date']).dt.year

# Contar la cantidad de datos por año
data_per_year = df_filtered['year'].value_counts().sort_index()

# Graficar los resultados
plt.figure(figsize=(10, 6))
data_per_year.plot(kind='bar')
plt.title('Cantidad de datos por año')
plt.xlabel('Año')
plt.ylabel('Cantidad de datos')
plt.show()

# Eliminar la columna temporal 'year'
df_filtered.drop(columns=['year'], inplace=True)

"""Las reseñas van desde el 2006 al 2022. Se puede ver que el numero de reseñas crece considerablemente para el año 2014, hasta el pico en 2018/2019, pero vuelven a caer en 2020."""

print(f'Min count: {data_per_year.min()} (Year: {data_per_year.idxmin()})')
print(f'Max count: {data_per_year.max()} (Year: {data_per_year.idxmax()})')
print(f'Average count: {data_per_year.mean().round(2)}')

"""El año que mas reseñas tiene es el 2019, y el que menos tiene es 2006 y el promedio de reseñas por año es de 697.

Como saber el año de la reseña no resulta relevante para el sistema de recomendacion, tambien optamos por eliminarla.
"""

df_filtered.drop(columns = ['date'], inplace = True)

"""### **attributes**
Una colección de atributos sobre el negocio, como 'Acepta tarjetas de crédito', 'Estacionamiento', 'Wi-Fi', etc.

# Preprocesamiento II de los datos:
- tokenizacion
- limpieza: eliminacion de mayusculas y caracteres alfanumericos
- homogenizacion: lematizacion
"""

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/TP Final IA/df_original.csv' # Add a filename to the path

# Exportar el DataFrame a un archivo CSV en Google Drive
df_filtered.to_csv(file_path, index=False)

print(f'Archivo guardado en: {file_path}')

"""# **Modelo**"""

import nltk
from nltk.stem.wordnet import WordNetLemmatizer
import string

# Descargar recursos de NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

"""### Analisis de frecuencia sin stop words"""

# Cargar módulos y datos
lemmatizer = WordNetLemmatizer()
stopwords_en = set(nltk.corpus.stopwords.words('english'))
punctuation = string.punctuation

def normalize(text):
    # Tokenizamos
    text_tokens = nltk.wordpunct_tokenize(text)

    # Pasamos a minúsculas y alfanumérico
    text_lower_alpha = [token.lower() for token in text_tokens if token.isalnum()]

    # Eliminamos tokens que contengan números
    text_no_numbers = [token for token in text_lower_alpha if not any(char.isdigit() for char in token)]

    # Eliminamos stop-words
    text_clean = [token for token in text_no_numbers if token not in stopwords_en]

    # Lematizamos
    normalized_text = [lemmatizer.lemmatize(word) for word in text_clean]

    return normalized_text

text_to_analyze = df_filtered.text
text_to_analyze

"""Normalizaremos el texto con la funcion normalize para luego hacer en análisis de las frecuencias"""

df_filtered['normalized_text'] = df_filtered['text'].apply(normalize)

df_filtered.normalized_text

tokens_corpus = [token for row in df_filtered.normalized_text for token in row]
counts  = nltk.FreqDist(tokens_corpus)
counts

vocab   = len(counts.keys())
words   = sum(counts.values())
lexdiv  = float(words) / float(vocab)

print("El corpus tiene %i palabras únicas y un total de %i palabras con una diversidad léxica de %0.3f" % (vocab, words, lexdiv))

counts.most_common(20)

counts.plot(40, cumulative=False)

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Create and generate a word cloud image:
wordcloud = WordCloud(max_font_size=50, max_words=40, background_color="white").generate(' '.join(counts))

# Display the generated image:
plt.figure(figsize=(10,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""Creamos una nueva columna en la que consideraremos a las reviews con puntaje de 4 o 5 como reviews positivas, las que tienen 1 o 2 como negativas, y aquellas que tienen 3 puntos como neutras. Estas ultimas no resultan utiles para el modelo porque no son ni positivas ni negativas, por lo que no las tendremos en cuenta."""

def score_target(score):
    if score == 5 or score == 4:
        return 1
    elif score == 2 or score == 1:
        return 0
    else:
        return None

df_filtered['Positive'] = df_filtered['stars_review'].apply(score_target)

df_filtered = df_filtered.dropna(subset=['Positive']).reset_index(drop=True)

df_filtered

df_filtered.Positive.value_counts()

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/TP Final IA/TP reseñas/df_filtered_brandon.csv' # Add a filename to the path

#Exportar el DataFrame a un archivo CSV en Google Drive
df_filtered.to_csv(file_path, index=False)

print(f'Archivo guardado en: {file_path}')